# What Is ChatGPT Doing … and Why Does It Work?

Stephen Wolfram, February 14, 2023

## It’s Just Adding One Word at a Time

ChatGPT 可以自动生成读起来甚至表面上像人类书写的文本，这一点非常了不起，也出乎意料。但它是如何做到的呢？它为什么能做到？我在这里的目的是粗略地介绍一下 ChatGPT 内部的情况，然后探讨它为什么能如此出色地生成我们认为有意义的文本。首先，我想说的是，我将专注于事情的全貌--虽然我会提到一些工程细节，但我不会深入探讨。（我所说内容的精髓同样适用于当前的其他 "大型语言模型"[LLMs]，也适用于 ChatGPT）。

首先要解释的是，ChatGPT 从根本上说一直在努力做的事情就是对目前所获得的文本进行 "合理的延续"，这里的 "合理 "指的是 "在看到人们在数十亿个网页上所写的内容等之后，人们可能期望某人写出的内容"。

比方说，我们有这样一段文字："The best thing about AI is its ability to"。想象一下，扫描数十亿页人类书写的文本（比如网络上和数字化书籍中的文本），然后找到这些文本的所有实例--然后再看看下一个词出现的频率是多少。ChatGPT 就能有效地做到这一点，只不过（正如我将解释的那样）它并不看文字的字面意思，而是寻找在某种意义上 "意义匹配 "的东西。但最终结果是，它会生成一个可能排在后面的词的排序列表以及 "概率"：

![Alt text](https://content.wolfram.com/sites/43/2023/02/sw021423img1.png)

最了不起的是，当 ChatGPT 做一些类似于写作文的事情时，它所做的本质上只是一遍又一遍地询问 "鉴于目前的文本，下一个词应该是什么？"--每次都添加一个词。（更准确地说，正如我将要解释的那样，它是在添加一个 "token"，这可能只是一个词的一部分，这就是为什么它有时会 "创造新词"）。

但是，好吧，在每个步骤中，它都会得到一个带概率的单词表。但它究竟应该选择哪个词添加到它正在写的文章（或其他东西）中呢？人们可能会认为应该是 "排名最高 "的词（即 "概率 "最高的词）。但就在这时，一些巫术开始悄然出现。因为出于某种原因--也许有一天我们会对这种原因有科学式的理解--如果我们总是选择排名最高的单词，我们通常会得到一篇非常 "平淡 "的作文，似乎从未 "表现出任何创造性"（甚至有时会逐字重复）。但如果有时（随意）选取排名较低的词，我们就会得到一篇 "更有趣 "的文章。

这里的随机性意味着，如果我们多次使用同一个提示，每次都可能得到不同的作文。而且，为了与巫术的理念保持一致，还有一个所谓的 "temperature" 参数，它决定了排名较低的词被使用的频率，而对于作文生成来说，0.8 的 "temperature" 似乎是最好的。（值得强调的是，这里并没有使用任何 "理论"；这只是一个在实践中行之有效的问题。举例来说，"温度 "的概念之所以存在，是因为恰好使用了统计物理学中熟悉的指数分布，但这与 "物理 "并无关联--至少就我们所知是这样）。

在我们继续之前，我应该解释一下，为了便于说明，我通常不会使用 ChatGPT 中的完整系统；相反，我通常会使用更简单的 GPT-2 系统，它有一个很好的特点，就是足够小，可以在标准的台式电脑上运行。因此，在我展示的所有内容中，我都会包含明确的 Wolfram 语言代码，您可以在自己的电脑上立即运行。(点击这里的任何图片，即可复制其背后的代码）。

例如，下面是获取上述概率表的方法。首先，我们必须检索底层的 "语言模型 "神经网络：

![Alt text](https://content.wolfram.com/sites/43/2023/02/sw021423img2.png)

稍后，我们将进入这个神经网络，了解它是如何工作的。但现在，我们只需将这个 "网络模型 "作为一个黑盒子，应用到我们目前的文本中，然后根据概率找出该模型认为应该遵循的前 5 个单词：

![Alt text](https://content.wolfram.com/sites/43/2023/02/sw021423img3.png)

这样就可以将结果转换成格式化的 "数据集"：

![Alt text](https://content.wolfram.com/sites/43/2023/02/sw021423img4.png)

下面是重复 "应用模型 "的结果--每一步都添加概率最高的单词（在此代码中指定为模型中的 "decision"）：

![Alt text](https://content.wolfram.com/sites/43/2023/02/sw021423img5.png)

如果时间再长一点会怎样？在这种（"zero temperature"）情况下，很快就会出现混乱和重复：

![Alt text](https://content.wolfram.com/sites/43/2023/02/sw021423img6.png)

但是，如果不总是选择 "顶级 "词，而是有时随机选择 "非顶级 "词（"随机性 "相当于 "temperature "0.8），又会怎样呢？同样可以建立文本：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img7.png)

而每次这样做，都会做出不同的随机选择，文本也会不同，就像这 5 个例子一样：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img8.png)

值得注意的是，即使在第一步，也有很多可能的 "下一个词 "可供选择（温度为 0.8），尽管它们的概率下降得很快（是的，对数图上的直线对应于 $n^{-1}$ 的 "幂律 "衰减，这是语言统计的一般特征）：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img10.png)

那么，如果时间更长会发生什么呢？下面是一个随机例子。这个例子比最前面那个词（zero temperature）的情况要好一些，但还是有点奇怪：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img11A.png)

这是用最简单的 GPT-2 型号（2019 年）完成的。使用更新、更大的 GPT-3 型号，效果会更好。下面是使用同样的 "提示"，但使用最大的 GPT-3 型号时产生的顶部字词（zero temperature）文本：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img12.png)

下面是一个 "温度 0.8 "的随机例子：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img13.png)

## Where Do the Probabilities Come From?

好了，ChatGPT 总是根据概率来选择下一个单词。但这些概率从何而来呢？让我们从一个更简单的问题开始。让我们考虑一次生成一个字母（而不是单词）的英文文本。我们如何计算出每个字母的概率呢？

我们可以做的最简单的事情就是抽取英文文本样本，计算不同字母在其中出现的频率。例如，计算维基百科中关于 "猫 "的文章中出现的字母：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img14-edit.png)

这对 "狗 "来说也是一样：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img15.png)

结果相似，但不尽相同（"o "无疑在 "dogs "冠词中更常见，因为毕竟它出现在 "dog "一词本身中）。尽管如此，如果我们抽取的英语文本样本足够多，我们还是可以期待最终得到至少相当一致的结果：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img16.png)

下面是我们用这些概率生成字母序列的示例：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img17.png)

我们可以通过添加空格将其分解为 "单词"，就像添加具有一定概率的字母一样：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img18.png)

我们可以通过迫使 "单词长度 "的分布与英语中的分布相一致，在制造 "单词 "方面做得更好一些：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img19.png)

我们在这里并没有得到任何 "actual words"，但结果看起来略有好转。不过，要想更进一步，我们需要做的不仅仅是随机挑选每个字母。举例来说，我们知道，如果有一个 "q"，那么下一个字母基本上就是 "u"。

下面是字母单独出现的概率图：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img20.png)

下面的图表显示了典型英语文本中成对字母（"2-grams"）的概率。可能出现的第一个字母显示在页面的横向，第二个字母显示在页面的纵向：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img21.png)

例如，我们可以看到，除了 "u "行之外，"q "列是空白的（概率为零）。好了，现在我们不再一次生成一个字母的 "单词"，而是使用这些 "2-gram "概率，一次生成两个字母的 "单词"。下面是一个结果样本--其中恰好包含了一些 "实际单词"：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img22.png)

有了足够多的英文文本，我们不仅可以对单个字母或字母对（2-grams）的概率，还可以对较长的字母组合的概率做出相当准确的估计。如果我们生成的 "随机单词 "的 n-gram 概率逐渐变长，我们就会发现它们逐渐变得 "more realistic"：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img23.png)

但是，让我们现在假设--或多或少像 ChatGPT 所做的那样--我们处理的是整个单词，而不是字母。英语中大约有 4 万个比较常用的单词。通过查看大量的英语文本语料库（比如几百万本书，共几千亿个单词），我们可以估算出每个单词的常用程度。利用这一点，我们就可以开始生成 "句子"，其中的每个单词都是独立随机抽取的，与出现在语料库中的概率相同。下面是我们得到的一个样本：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img24.png)

毫不奇怪，这是无稽之谈。那么，我们怎样才能做得更好呢？就像处理字母一样，我们不仅可以开始考虑单词的概率，还可以考虑词对或更长的 n-gram 的概率。下面是我们从 "cat"（猫）这一单词开始得到的 5 个例子：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img25.png)

它看起来越来越 "合理 "了。我们可以想象，如果我们能够使用足够长的 n-gram，我们基本上就能 "得到一个 ChatGPT"--也就是说，我们会得到一个东西，它能生成具有 "正确的整体作文概率 "的作文长单词序列。但问题是：我们根本没有足够多的英语文章来推导出这些概率。

在对网络的抓取中，可能有几千亿个单词；在已经数字化的书籍中，可能又有几千亿个单词。但是，即使是 40,000 个常用词，可能出现的 2-grams 的数量也已经达到了 16 亿，而可能出现的 3-grams 的数量更是高达 60 万亿。因此，我们根本无法从现有的文本中估算出所有这些词的概率。而到了 20 个单词的 "文章片段 "时，可能性的数量已经超过了宇宙中粒子的数量，所以从某种意义上说，我们永远不可能把它们全部写下来。

那么我们能做些什么呢？最重要的想法是建立一个模型，让我们能够估算出序列出现的概率--即使我们从未在所查看的文本语料库中明确看到过这些序列。而 ChatGPT 的核心正是一个所谓的 "大型语言模型"（LLM），它的建立可以很好地估计这些概率。

## What Is a Model?

假设你想知道（就像伽利略在 15 世纪晚期所做的）从比萨斜塔的每一层投下一颗炮弹到落地需要多长时间。那么，你可以测量每种情况下的时间，并将结果列成表格。或者，你也可以采用理论科学的精髓：建立一个模型，给出计算答案的某种程序，而不仅仅是测量和记住每种情况。

假设我们有炮弹从不同楼层落下所需的时间数据（有点理想化）：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img26.png)

我们如何计算出从我们没有明确数据的地板上掉下来需要多长时间？在这种特殊情况下，我们可以利用已知的物理定律来计算。但是，如果我们只有数据，而不知道有什么基本定律支配着它。那么，我们可以做一个数学上的猜测，比如，也许我们应该用一条直线作为模型：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img27.png)

我们可以选择不同的直线。但这是平均而言最接近所给数据的一条直线。根据这条直线，我们可以估算出任何楼层的下降时间。

我们怎么会知道在这里使用直线呢？在某种程度上，我们并不知道。这只是数学上很简单的东西，而我们已经习惯了这样一个事实，即我们测量的很多数据都被数学上简单的东西很好地拟合了。我们可以尝试一些数学上更复杂的方法--比如说 $a + bx + cx^2$ --然后在这种情况下，我们会做得更好：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img29.png)

不过也有可能出错。比如，我们用 $a + b/x + c\sin(x)$ 的话最好只能做成这样：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img33.png)

值得理解的是，从来没有 "model-less model"。你所使用的任何模型都有一些特定的底层结构，然后有一组 "你可以转动的旋钮"（即你可以设置的参数）来适应你的数据。在 ChatGPT 中，使用了大量这样的 "旋钮"--实际上有 1750 亿个。

但难能可贵的是，ChatGPT 的底层结构--"仅仅 "有这么多参数--就足以建立一个模型，"足够好地 "计算下一个词的概率，从而为我们提供合理的长篇文章。

## Models for Human-Like Tasks

我们上面举的例子涉及为数字数据建立模型，而数字数据基本上来自简单物理学--我们几个世纪以来就知道 "简单数学适用"。但对于 ChatGPT，我们必须建立一个由人脑生成的人类语言文本模型。而对于这样的东西，我们（至少目前）还没有类似于 "简单数学 "的东西。那么，它的模型会是什么样的呢？

在谈语言之前，我们先来谈谈另一项类似人类的任务：识别图像。作为一个简单的例子，让我们来看看数字图像（没错，这是一个典型的机器学习例子）：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img34.png)

我们可以做的一件事是为每个数字获取大量样本图像：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img35.png)

那么，要想知道我们输入的图像是否对应于某个特定的数字，我们只需与现有的样本进行明确的逐像素比较即可。但作为人类，我们似乎可以做得更好--因为我们仍然可以识别数字，即使它们是手写的，而且有各种修改和变形：

![](https://content.wolfram.com/sites/43/2023/03/sw021423img36-4.png)

当我们为上面的数字数据建立模型时，我们可以利用给定的数值 $x$，针对特定的 $a$ 和 $b$ 计算出 $a + b x$。那么，如果我们把这里每个像素的灰度值看作某个变量 $x_i$，是否所有这些变量的某个函数在求值时都能告诉我们图像对应哪个数字？事实证明，构建这样一个函数是可能的。不过，它并不是特别简单，这一点也不奇怪。一个典型的例子可能涉及 50 万次数学运算。

但最终的结果是，如果我们将图像的像素值集合输入这个函数，就会得到一个数字，说明我们得到的是哪位数字的图像。稍后，我们将讨论如何构建这样一个函数，以及神经网络的概念。但现在，让我们把这个函数当作一个黑盒子，输入手写数字的图像（像素值数组），然后得到这些图像对应的数字：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img38.png)

但这究竟是怎么回事呢？假设我们逐渐模糊一个数字。有一小段时间，我们的函数仍能 "识别 "它，这里是 "2"。但很快它就 "失去了它"，开始给出 "错误 "的结果：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img39.png)

但为什么我们说这是 "错误 "的结果呢？在这种情况下，我们知道我们通过模糊 "2 "得到了所有图像。但是，如果我们的目标是建立一个人类识别图像的模型，那么真正要问的问题是，如果人类在不知道图像来源的情况下看到其中一张模糊的图像，会怎么做。

如果我们从函数中得到的结果通常与人类的说法一致，那么我们就有了一个 "好模型"。而一个非同小可的科学事实是，对于像这样的图像识别任务，我们现在基本上已经知道如何构建能做到这一点的函数。

我们能 "从数学上证明 "它们有效吗？嗯，不能。因为要做到这一点，我们必须对人类的行为有一套数学理论。以 "2 "图像为例，改变几个像素。我们可能会想，如果只有几个像素 "错位"，我们还是应该把图像看作 "2"。但这应该做到什么程度呢？这是一个人类视觉感知的问题。是的，对于蜜蜂或章鱼来说，答案无疑是不同的，而对于假想的外星人来说，答案也可能完全不同。

## Neural Nets

那么，我们用于图像识别等任务的典型模型究竟是如何工作的呢？目前最流行也是最成功的方法是使用神经网络。神经网络发明于 20 世纪 40 年代，其形式与今天的神经网络非常接近，可以看作是大脑工作方式的简单理想化。

人脑中约有 1,000 亿个神经元（神经细胞），每个神经元每秒能产生大约 1000 次电脉冲。这些神经元连接成一个复杂的网络，每个神经元都有树状分支，可以将电信号传递给成千上万个其他神经元。粗略估计，任何一个神经元在某一时刻是否会产生电脉冲，取决于它从其他神经元接收到的脉冲信号--不同的连接会产生不同的 "权重"。

当我们 "看到图像 "时，图像中的光子落到我们眼球后部的细胞（"感光器"）上，就会在神经细胞中产生电信号。这些神经细胞与其他神经细胞相连，最终这些信号会通过一连串的神经元层。正是在这个过程中，我们 "识别 "了图像，最终 "形成了 "我们 "看到了一个 2 "的想法（也许最后我们会做一些事情，比如大声说出 "2 "这个词）。

上一节中的 "黑盒 "函数就是这种神经网络的 "数学化 "版本。它恰好有 11 层（虽然只有 4 个 "核心层"）：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img40A.png)

这个神经网络并没有什么特别的 "理论来源"；它只是在 1998 年作为一项工程被构建出来，并被发现可以工作的东西。（当然，这与我们将大脑描述为生物进化过程中产生的并无太大区别）。

好吧，但这样的神经网络是如何 "识别事物 "的呢？关键在于 attractors 的概念。想象一下，我们有手写的 "1 "和 "2 "的图像：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img41.png)

我们希望所有的 1 都 "被吸引到一个地方"，而所有的 2 都 "被吸引到另一个地方"。或者换一种说法，如果一个图像 "更接近 1 "而不是 "更接近 2"，我们希望它最终出现在 "1 的位置"，反之亦然。

打个简单的比方，假设我们在平面上有一些用点表示的位置（在现实生活中，这些点可能是咖啡店的位置）。那么我们可以想象，从平面上的任何一点出发，我们总是希望到达最近的点（即我们总是去最近的咖啡店）。我们可以将平面划分为若干区域（"吸引盆地 -- attractor basins"），这些区域被理想化的 "分水岭 -- watersheds"分隔开来：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img42.png)

我们可以把这看作是在执行一种 "识别任务"，在这项任务中，我们并不是在识别给定图像 "最像 "哪位数字，而是直接查看给定点最接近哪个点。（我们在这里展示的 "Voronoi diagram "设置是在二维欧几里得空间中将点分开；数字识别任务可以被认为是在做非常类似的事情--但却是在由每幅图像中所有像素的灰度级组成的 784 维空间中）。

那么，我们该如何让神经网络 "完成识别任务 "呢？让我们来看看这个非常简单的例子：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img43.png)

我们的目标是获取与位置 {x,y} 相对应的 "输入"，然后将其 "识别 "为最接近的三个点中的任何一个。换句话说，我们希望神经网络能计算出 {x,y} 的函数，比如

![](https://content.wolfram.com/sites/43/2023/02/sw021423img44.png)

那么，我们如何利用神经网络来实现这一目标呢？归根结底，神经网络是理想化 "神经元 "的连接集合--通常分层排列--一个简单的例子就是：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img45.png)

每个 "神经元 "实际上都是用来评估一个简单的数字函数。要 "使用 "这个网络，我们只需在顶层输入数字（如坐标 x 和 y），然后让每一层的神经元 "评估它们的函数"，并将结果通过网络向前传递--最终在底层产生最终结果：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img46.png)

在传统的（受生物学启发的）设置中，每个神经元实际上都有一组来自上一层神经元的 "输入连接"，每个连接都被赋予一定的 "权重"（可以是正数或负数）。将 "前一个神经元 "的值乘以相应的权重，然后相加，再加上一个常数，最后应用一个 "阈值 -- thresholding"（或 "激活 -- activation"）函数，就能确定某个神经元的值。用数学术语来说，如果一个神经元的输入为 $x = {x1、x2 ... }$，那么我们就可以计算 $f[w . x + b]$，其中权重 $w$ 和常数 $b$ 通常是为网络中的每个神经元选择的，但函数 $f$ 通常是相同的。

计算 $w . x + b$ 只需矩阵乘法和加法。激活函数 " $f$ 引入了非线性（并最终导致了非凡的行为）。通常会使用各种激活函数，这里我们只使用 Ramp（或 ReLU）：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img48.png)

对于我们希望神经网络执行的每项任务（或者说，对于我们希望它评估的每个整体功能），我们都会有不同的权重选择。（正如我们稍后将讨论的那样，这些权重通常是通过使用机器学习的方法从我们想要的输出示例中 "训练 "神经网络来确定的）。

归根结底，每个神经网络都对应着某种整体数学函数--虽然写出来可能很乱。对于上面的例子，它应该是：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img49.png)

ChatGPT 的神经网络也只是对应于这样一个数学函数，但实际上有数十亿个项。

不过，让我们回到单个神经元上来。下面是一些例子，说明神经元在有两个输入（代表坐标 x 和 y）的情况下，通过选择不同的权重和常数（以及作为激活函数的 Ramp）可以计算的函数：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img50.png)

但从上面看，更大的网络又是怎样的呢？下面是它的计算结果：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img51.png)

虽然还不是很 "正确"，但已经接近我们上面展示的 "最近点 "函数了。

让我们看看其他神经网络的情况。在每种情况下，正如我们稍后要解释的，我们都使用机器学习来找到最佳的权重选择。然后，我们在这里展示使用这些权重的神经网络的计算结果：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img52.png)

更大的网络通常能更好地逼近我们的目标函数。在 "每个吸引子盆地 (attractor basin) 的中间"，我们通常能得到我们想要的答案。但在边界--神经网络 "很难下定决心 "的地方--情况可能会更加混乱。

在这个简单的数学式 "识别任务 "中，"正确答案 "是什么一目了然。但在识别手写数字的问题上，就不那么清楚了。如果有人把 "2 "写得像 "7 "怎么办？尽管如此，我们仍然可以问神经网络是如何区分数字的--这给出了一个提示：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img53.png)

我们能 "从数学角度 "说明网络是如何进行区分的吗？并不能。它只是 "做了神经网络所做的事"。但事实证明，通常情况下，这似乎与我们人类所做的区分相当吻合。

让我们举一个更详细的例子。假设我们有猫和狗的图像。我们有一个训练有素的神经网络来区分它们。下面是它在一些例子中的表现：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img54.png)

现在，"正确答案 "是什么就更不清楚了。一只穿着猫咪衣服的狗怎么样？等等。无论输入什么信息，神经网络都会产生一个答案，而且答案的方式与人类的方式相当一致。正如我在上文所说，这并不是我们可以 "从第一原理中推导出 "的事实。至少在某些领域，这只是经验证明的事实。但这正是神经网络有用的一个关键原因：它们以某种方式捕捉到了 "类似人类 "的做事方式。

给自己看一张猫的照片，然后问 "为什么那是一只猫？也许你会说 "嗯，我看到了它的尖耳朵等等"。但要解释你是如何认出那是一只猫并不容易。只是你的以某种方式想到了这一点。但对于大脑来说，我们没有办法（至少现在还没有办法）"进入 "它的内部，看看它是怎么想出来的。那么（人工）神经网络呢？当你展示一张猫的图片时，可以直接看到每个 "神经元 "在做什么。但即使是获得基本的可视化效果，通常也非常困难。

在我们用于上述 "最近点 "问题的最终网络中，有 17 个神经元。在识别手写数字的网络中，有 2190 个神经元。而在我们用来识别猫和狗的网络中，有 60650 个神经元。通常情况下，要将 60650 个维度的空间可视化是非常困难的。但由于这是一个为处理图像而建立的网络，它的许多神经元层都被组织成数组，就像它正在观察的像素数组一样。

如果我们以一只典型的猫为例

![](https://content.wolfram.com/sites/43/2023/02/sw021423img55.png)

那么，我们就可以用一组衍生图像来表示第一层神经元的状态--我们可以很容易地将其中许多图像解释为 "没有背景的猫 "或 "猫的轮廓"：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img56.png)

到了第 10 层，就很难解释发生了什么：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img57.png)

但一般来说，我们可以说神经网络是在 "挑选出某些特征"（也许尖耳朵就是其中之一），并利用这些特征来确定图像的内容。但是，这些特征是我们有名称的特征吗，比如 "尖耳朵"？大多不是。

我们的大脑在使用类似的功能吗？大多数情况下我们并不清楚。但值得注意的是，像我们在这里展示的这种神经网络的前几层似乎能识别出图像的某些方面（如物体的边缘），这似乎与我们所知的大脑第一层视觉处理所识别出的特征相似。

但是，假设我们想要一个神经网络的 "猫识别理论"。我们可以说"看，这个特定的网络就能做到这一点"--这立刻就能让我们感觉到这是一个 "有多难的问题"（例如，可能需要多少个神经元或多少层）。但至少到目前为止，我们还没有办法 "娓娓道来 "地描述网络在做什么。也许这是因为它在计算上确实是不可还原的，除了明确地追踪每一步之外，没有一般的方法可以找到它在做什么。或者，这只是因为我们还没有 "搞清楚这门科学"，还没有找出 "自然法则 "来让我们概括发生了什么。

当我们讨论用 ChatGPT 生成语言时，也会遇到同样的问题。同样，我们也不清楚是否有办法 "总结它在做什么"。但语言的丰富性和细节（以及我们的经验）可能会让我们比图像走得更远。

## Machine Learning, and the Training of Neural Nets

到目前为止，我们一直在讨论 "已经知道 "如何完成特定任务的神经网络。但神经网络之所以如此有用（大概也适用于大脑），是因为它们不仅原则上可以完成各种任务，而且还可以逐步 "通过实例训练 "来完成这些任务。

当我们制作一个神经网络来区分猫和狗时，我们实际上并不需要编写一个程序来（比如说）明确地找到胡须；相反，我们只需要展示大量关于什么是猫什么是狗的例子，然后让网络从这些例子中 "机器学习 "如何区分它们。

问题的关键在于，训练有素的网络能从所展示的特定例子中 "归纳 "出来。正如我们在上文所看到的，这并不是说神经网络能够识别出所展示的猫咪图像中的特定像素模式，而是说神经网络能够根据我们所认为的某种 "general catness" 来区分图像。

那么，神经网络训练究竟是如何进行的呢？从根本上说，我们一直在努力寻找能让神经网络成功重现我们给出的例子的权重。然后，我们依靠神经网络以 "合理 "的方式在这些例子之间进行 "插值"（或 "泛化"）。

让我们来看一个比上述最近点问题更简单的问题。让神经网络来学习函数：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img58.png)

为此，我们需要一个只有一个输入和一个输出的网络，例如

![](https://content.wolfram.com/sites/43/2023/02/sw021423img59.png)

但我们应该使用什么样的权重呢？每给一组可能的权重，神经网络都会计算对应的函数。举例来说，下面是随机选择的几组权重的计算结果：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img60.png)

是的，我们可以清楚地看到，在所有这些情况下，它都无法重现我们想要的函数。那么，我们该如何找到能重现函数的权重呢？

其基本思想是提供大量的 "输入 → 输出 "示例来进行 "学习"，然后尝试找到能够重现这些示例的权重。下面就是用逐渐增多的示例进行学习的结果：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img61.png)

在 "训练 "的每个阶段，网络中的权重都会逐步调整--我们看到，最终我们得到的网络能够成功地再现我们想要的功能。那么，我们该如何调整权重呢？我们的基本思路是在每个阶段查看离我们想要的功能 "还有多远"，然后更新权重，使其更接近我们想要的功能。

为了找出 "我们离得有多远"，我们要计算通常所说的 "损失函数 -- loss function"（有时也称为 "代价函数 -- cost function"）。在这里，我们使用的是一个简单的（L2）损失函数，它只是我们得到的值与真实值之差的平方和。我们可以看到，随着训练过程的进行，损失函数会逐渐减小（遵循一定的 "学习曲线"，不同的任务有不同的学习曲线）--直到我们达到一个点，网络（至少是一个很好的近似值）成功地再现了我们想要的函数：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img62.png)

好了，最后要解释的是如何调整权重以减小损失函数。正如我们所说，损失函数给出了我们得到的值与真实值之间的 "距离"。但是，"我们得到的值 "在每个阶段都是由当前版本的神经网络和其中的权重决定的。但现在想象一下，权重是变量，比如 $w_i$。我们想知道如何调整这些变量的值，以最小化取决于它们的损失。

例如，想象一下（对实际应用中的典型神经网络进行惊人的简化），我们只有两个权重 $w_1$ 和 $w_2$。那么我们可能会有这样一个损失，它是 $w_1$ 和 $w_2$ 的函数：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img68.png)

在这种情况下，数值分析提供了多种寻找最小值的技术。不过，一种典型的方法是从之前的 $w_1$ 、$w_2$ 开始，沿着最陡峭下降的路径逐步求取：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img71.png)

就像水从山上流下一样，我们只能保证这个过程最终会到达曲面的某个局部最小值（"高山湖泊"），而很可能达不到最终的全局最小值。

要在 "权重景观 "上找到最陡峭的下降路径，并不是明显可行的。不过，微积分可以帮上忙。如上所述，我们可以将神经网络看作是在计算一个数学函数--它取决于输入和权重。但现在我们可以考虑对这些权重进行微分。事实证明，微积分的链式法则实际上可以让我们 "解开 "神经网络中连续各层所做的运算。其结果是，我们可以--至少在某些局部近似情况下--"反转 "神经网络的操作，并逐步找到权重值，使输出相关的损失最小化。

上图显示了在只有 2 个权重的不切实际的简单情况下，我们可能需要做的最小化。但事实证明，即使使用更多的权重（ChatGPT 使用了 1,750 亿个权重），我们仍有可能实现最小化，至少在某种程度上是近似的。**事实上，"深度学习 "在 2011 年前后取得的重大突破，就是因为人们发现，从某种意义上说，当权重较多时，比权重较少时更容易实现最小化（至少是近似最小化）。**

换句话说--有点违背直觉--用神经网络解决更复杂的问题比解决更简单的问题更容易。这其中的大致原因似乎是，当 "权重变量 "较多时，就会产生一个高维空间，其中有 "许多不同的方向 "可以引导我们找到最小值，而当变量较少时，就很容易陷入局部最小值（"高山湖泊"），没有 "出路"。

值得指出的是，在通常情况下，有许多不同的权重集合都能使神经网络获得基本相同的性能。而在实际的神经网络训练中，通常会有很多随机选择，从而产生 "不同但等效的解决方案"，就像这些：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img72.png)

但是，每一种 "不同的解决方案 "至少会有略微不同的行为。而且，如果我们要求在我们提供训练示例的区域之外进行 "外推法"，我们可能会得到截然不同的结果：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img73.png)

但哪一种才是 "正确 "的呢？真的无从说起。它们都 "符合观察到的数据"。但它们都对应着不同的 "先天""思维 "方式，即 "跳出框框 "来 "思考 "该怎么做。在我们人类看来，有些可能比另一些 "更合理"。

## The Practice and Lore of Neural Net Training

特别是在过去十年中，神经网络训练的艺术取得了许多进步。没错，这基本上就是一门艺术。有时--尤其是回过头来看--人们至少能看到 "科学解释 "的曙光。但大多数情况下，我们都是通过不断尝试和犯错来发现问题，不断添加新的想法和技巧，从而逐步建立起关于如何使用神经网络的重要知识（lore）。

有几个关键部分。首先是针对特定任务应该使用哪种结构的神经网络。然后是如何获得训练神经网络的数据这一关键问题。越来越多的情况下，我们并不是要从头开始训练一个神经网络：相反，一个新的神经网络可以直接吸收另一个已经训练过的神经网络，或者至少可以利用那个神经网络为自己生成更多的训练实例。

人们可能会认为，对于每一种特定的任务，都需要不同结构的神经网络。但我们发现，即使是表面上截然不同的任务，相同的架构似乎也能奏效。在某种程度上，这让人想起通用计算的概念（以及我的计算等价原则），但正如我稍后将要讨论的，我认为这更多地反映了一个事实，即我们通常试图让神经网络完成的任务是 "类人 "任务，而神经网络可以捕捉到相当普遍的 "类人过程"。

在神经网络发展的早期，人们往往认为应该 "让神经网络做尽可能少的事情"。例如，在将语音转换为文本时，人们认为应该首先分析语音音频，将其分解为音素等。但人们发现，至少对于 "类人任务 "来说，通常最好是尝试训练神经网络处理 "端到端问题"，让它自己 "发现 "必要的中间特征、编码等。

还有人认为，应该在神经网络中引入复杂的单个组件，让它实际上 "明确实现特定的算法思想"。但事实再次证明，这在大多数情况下是不值得的；相反，最好的办法是处理非常简单的组件，让它们 "自我组织"（尽管通常是以我们无法理解的方式），以实现（大概）这些算法思想。

这并不是说没有与神经网络相关的 "结构思想"。因此，举例来说，具有局部连接的二维神经元阵列至少在处理图像的早期阶段非常有用。在处理人类语言等问题时，例如在 ChatGPT 中，具有集中于 "序列回溯 "的连接模式似乎也很有用--我们稍后会看到这一点。

但神经网络的一个重要特点是，与一般计算机一样，它们最终只是在处理数据。而目前的神经网络--目前的神经网络训练方法--专门处理数字阵列。但在处理过程中，这些数组可以完全重新排列和重塑。举例来说，我们上面用于识别数字的网络从二维 "图像 "阵列开始，迅速 "增厚 "到多个通道，然后 "缩小 "到一维阵列，最终包含代表不同可能输出数字的元素：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img74.png)

但是，好吧，怎样才能知道特定任务需要多大的神经网络呢？这是一门艺术。在某种程度上，关键是要知道 "任务有多难"。但对于类似人类的任务来说，这通常很难估算。是的，也许有一种系统的方法可以让计算机非常 "机械 "地完成任务。但我们很难知道，是否存在人们可能认为的窍门或捷径，能让我们至少在 "类人水平 "上更轻松地完成任务。要 "机械地 "玩某个游戏，可能需要枚举一棵巨大的游戏树；但要达到 "人类水平的游戏"，可能有更简单（"启发式"）的方法。

在处理微小神经网络和简单任务时，有时我们可以清楚地看到 "无法从这里到达那里"。例如，在上一节的任务中，我们用几个小神经网络来完成这个任务，似乎最好只能做到这样：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img75.png)

我们发现，如果网络太小，就无法再现我们想要的功能。但超过一定的规模，它就没有问题了--至少只要训练的时间足够长，训练的例子足够多。顺便说一句，这些图片说明了神经网络的一个传说：如果中间有一个 "挤压"，迫使所有东西都要经过较少的中间神经元数量，那么通常就可以使用较小的网络。（值得一提的是，"无中间层"--或所谓的 "感知器"--网络只能学习基本的线性函数，但只要有一个中间层，原则上就可以任意逼近任何函数，至少只要有足够多的神经元。尽管为了使其具有可训练性，通常需要进行某种正则化或归一化处理）

好了，假设我们已经确定了某种神经网络架构。现在的问题是如何获得训练网络所需的数据。围绕神经网络和机器学习的许多实际挑战都集中在获取或准备必要的训练数据上。在许多情况下（"监督学习"），人们希望获得输入和预期输出的明确示例。因此，举例来说，人们可能希望通过图像中的内容或其他属性来标记图像。也许我们需要明确地去做标记，通常要花费很大的力气。但很多时候，我们可以利用已经完成的工作，或者将其作为某种代理。例如，我们可以使用网络上为图片提供的alt标签。或者，在另一个领域，我们可以使用为视频制作的封闭式字幕。或者，在语言翻译训练中，我们可以使用不同语言版本的平行网页或其他文档。

你需要向神经网络展示多少数据才能训练它完成特定任务？同样，这也很难根据第一原理进行估算。当然，通过使用 "迁移学习 "来 "迁移 "另一个网络已经学习过的重要特征列表等，可以大大降低要求。但一般来说，神经网络需要 "看到大量示例 "才能得到良好的训练。**至少对于某些任务来说，神经网络的一个重要经验是，例子可以重复得令人难以置信。事实上，向神经网络展示所有示例是一种标准策略，而且是一遍又一遍地重复。**在每一轮 "训练"（或 "epochs"）中，神经网络至少会处于略微不同的状态，而以某种方式 "提醒它 "某个特定的例子，有助于让它 "记住那个例子"。（是的，这也许类似于人类记忆中重复的作用）。

但是，仅仅重复同样的例子往往是不够的。还需要向神经网络展示这些示例的变化。神经网络的一个特点是，这些 "数据增强 "变化并不一定要很复杂才有用。只需用基本的图像处理方法对图像稍作修改，就能使其在神经网络训练中 "焕然一新"。同样，当用于训练自动驾驶汽车的实际视频等耗尽时，我们可以继续在一个类似于视频游戏的模型环境中运行模拟，从中获取数据，而不需要真实世界场景的所有细节。

像 ChatGPT 这样的软件怎么样？它有一个很好的特点，就是可以进行 "无监督学习"，从而更容易获得训练用的示例。回想一下，ChatGPT 的基本任务是找出如何续写一段给定的文本。因此，要让它获得 "训练示例"，只需获取一段文本，屏蔽掉文本结尾，然后将其作为 "训练输入"--"输出 "则是完整的、未屏蔽的文本。我们稍后会详细讨论这个问题，但重点是，与学习图像中的内容不同，不需要 "明确标记"；ChatGPT 实际上可以直接从给定的文本示例中学习。

那么，神经网络的实际学习过程又是怎样的呢？归根结底，就是要确定什么样的权重能最好地捕捉所给的训练示例。有各种各样的细节选择和 "超参数设置"（之所以称为 "超参数设置"，是因为权重可以被看作是 "参数"），可以用来调整学习过程。损失函数有不同的选择（平方和、绝对值总和等）。损失最小化有不同的方法（每一步在权重空间中移动多远等）。还有一些问题，比如需要展示多大 "批量 "的示例，才能获得试图最小化的损失的每次连续估计值。**是的，我们可以应用机器学习（例如，我们在 Wolfram 语言中就是这样做的）来自动进行机器学习，并自动设置超参数等。**

但最终，整个训练过程可以通过观察损失是如何逐步减少来体现（如 Wolfram 语言对一个小训练的进度监控）：

![](https://content.wolfram.com/sites/43/2023/02/sw021423img76.png)

我们通常会看到，损失会在一段时间内减少，但最终会在某个恒定值上趋于平缓。如果这个值足够小，那么就可以认为训练是成功的；否则，这可能是一个信号，表明我们应该尝试改变网络架构。

能知道 "学习曲线 "需要多长时间才能趋于平缓吗？就像许多其他事情一样，似乎存在近似的幂律比例关系，这取决于神经网络的大小和使用的数据量。但总的结论是，训练神经网络很难--需要耗费大量的计算精力。而实际上，绝大多数的计算工作都是在对数组进行运算，而这正是 GPU 所擅长的--这就是为什么神经网络训练通常会受到 GPU 可用性的限制。

未来，是否会有从根本上更好的方法来训练神经网络，或者做神经网络能做的事情？我认为几乎可以肯定。神经网络的基本理念是利用大量简单（基本相同）的组件创建一个灵活的 "计算结构"，而且这个 "结构 "可以逐步修改，以便从实例中学习。在当前的神经网络中，人们基本上是在使用微积分的思想--应用于实数--来进行增量修改。但越来越明显的是，拥有高精度的数字并不重要；即使使用目前的方法，8 bits 或更少可能就足够了。

像元胞自动机这样的计算系统，基本上是在许多单独的比特上并行操作的，如何进行这种增量修改一直都不清楚，但没有理由认为不可能。事实上，就像 "2012 年深度学习的突破 "一样，这种增量修改在更复杂的情况下可能比在简单的情况下更容易。

神经网络--或许有点像大脑--被设定为拥有一个基本固定的神经元网络，改变的是它们之间连接的强度（"权重"）。（也许至少在年轻的大脑中，大量全新的连接也可以生长）。不过，虽然这可能是一种方便的生物学设置，但我们并不清楚这是否是实现我们所需的功能的最佳方法。而一种相当于渐进式网络重写的方法（也许会让人想起我们的物理项目）最终可能会更好。

但即使是在现有的神经网络框架内，目前也存在一个关键的限制：目前的神经网络训练从根本上说是按顺序进行的，每批实例的效果都会传播回去，以更新权重。事实上，在目前的计算机硬件条件下，即使考虑到 GPU，神经网络在训练过程中的大部分时间都是 "闲置 "的，每次只更新一部分。从某种意义上说，这是因为我们目前的计算机往往拥有独立于 CPU（或 GPU）的内存。但在大脑中，情况可能有所不同--每一个 "内存元素"（即神经元）同时也是一个潜在的活跃计算元素。如果我们能以这种方式设置未来的计算机硬件，就有可能更高效地进行训练。

## “Surely a Network That’s Big Enough Can Do Anything!”

待续。





## 参考

Wolfram, S. (2023, February 14). What is ChatGPT doing ... and why does it work?. Stephen Wolfram Writings. writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work.